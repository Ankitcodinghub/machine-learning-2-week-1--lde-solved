# machine-learning-2-week-1--lde-solved
**TO GET THIS SOLUTION VISIT:** [Machine Learning 2 Week 1- LDE Solved](https://www.ankitcodinghub.com/product/machine-learning-2-week-1-lde-solved/)


---

📩 **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
📱 **WhatsApp:** +1 419 877 7882  
📄 **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;98811&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;0&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;0&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;0\/5 - (0 votes)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;Machine Learning 2 Week 1- LDE Solved&quot;,&quot;width&quot;:&quot;0&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 0px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            <span class="kksr-muted">Rate this product</span>
    </div>
    </div>
<div class="page" title="Page 1">
<div class="layoutArea">
<div class="column">
&nbsp;

&nbsp;

</div>
<div class="column">
Exercise Sheet 1

</div>
</div>
<div class="layoutArea">
<div class="column">
Exercise 1: Symmetries in LLE (25 P)

The Locally Linear Embedding (LLE) method takes as input a collection of data points ⃗x1,…,⃗xN ∈ Rd and embeds them in some low-dimensional space. LLE operates in two steps, with the first step consisting of minimizing the objective

􏰄N􏰀 􏰄 􏰀2 E(w) = 􏰀􏰀⃗xi − wij⃗xj􏰀􏰀

i=1 j

where w is a collection of reconstruction weights subject to the constraint ∀i : 􏰃j wij = 1, and where 􏰃j sums over the K nearest neighbors of the data point ⃗xi. The solution that minimizes the LLE objective can be shown to be invariant to various transformations of the data.

Show that invariance holds in particular for the following transformations:

<ol>
<li>(a) &nbsp;Replacement of all ⃗xi with α⃗xi, for an α ∈ R+ \ {0},</li>
<li>(b) &nbsp;Replacement of all ⃗xi with ⃗xi + ⃗v, for a vector ⃗v ∈ Rd,</li>
<li>(c) &nbsp;Replacement of all ⃗xi with U⃗xi, where U is an orthogonal d × d matrix.
Exercise 2: Closed form for LLE (25 P)

In the following, we would like to show that the optimal weights w have an explicit analytic solution. For this, we first observe that the objective function can be decomposed as a sum of as many subobjectives as there are data points:
</li>
</ol>
</div>
</div>
<div class="layoutArea">
<div class="column">
􏰄N E(w) =

</div>
<div class="column">
􏰀 􏰄 􏰀2 Ei(w) = 􏰀􏰀⃗xi − wij⃗xj􏰀􏰀

</div>
</div>
<div class="layoutArea">
<div class="column">
Ei(w)

Furthermore, because each subobjective depends on different parameters, they can be optimized indepen-

dently. We consider one such subobjective and for simplicity of notation, we rewrite it as:

􏰀 􏰄K 􏰀2

</div>
</div>
<div class="layoutArea">
<div class="column">
i=1

</div>
<div class="column">
j

</div>
</div>
<div class="layoutArea">
<div class="column">
with

</div>
</div>
<div class="layoutArea">
<div class="column">
E i ( w ) = 􏰀􏰀 ⃗x −

where ⃗x is the current data point (we have dropped the index i), where η = (⃗η1, . . . , ⃗ηK ) is a matrix of size K × d containing the K nearest neighbors of ⃗x, and w is the vector of size K containing the weights to optimize and subject to the constraint 􏰃Kj=1 wj = 1.

(a) Prove that the optimal weights for ⃗x are found by solving the following optimization problem: min w⊤Cw subject to w⊤1 = 1.

w

where C = (1⃗x⊤ − η)(1⃗x⊤ − η)⊤ is the covariance matrix associated to the data point ⃗x and 1 is a vector of ones of size K.

<ol start="2">
<li>(b) &nbsp;Show using the method of Lagrange multipliers that the minimum of the optimization problem found in (a) is given analytically as:
w= C−11 . 1⊤C−11
</li>
<li>(c) &nbsp;Show that the optimal w can be equivalently found by solving the equation Cw = 1 and then rescaling w such that w⊤1 = 1.</li>
</ol>
</div>
</div>
<div class="layoutArea">
<div class="column">
j=1

</div>
</div>
<div class="layoutArea">
<div class="column">
w j ⃗η j 􏰀􏰀

</div>
</div>
</div>
<div class="page" title="Page 2">
<div class="layoutArea">
<div class="column">
Exercise 3: SNE and Kullback-Leibler Divergence (25 P)

SNE is an embedding algorithm that operates by minimizing the Kullback-Leibler divergence between two discrete probability distributions p and q representing the input space and the embedding space respectively. In ‘symmetric SNE’, these discrete distributions assign to each pair of data points (i,j) in the dataset the probability scores pij and qij respectively, corresponding to how close the two data points are in the input and embedding spaces. Once the exact probability functions are defined, the embedding algorithm proceeds by optimizing the function:

C = DKL(p∥q)

N N 􏰁pij􏰂 = 􏰄􏰄pij log qij

i=1 j=1

where p and q are subject to the constraints 􏰃Ni=1 􏰃Nj=1 pij = 1 and 􏰃Ni=1 􏰃Nj=1 qij = 1. Specifically, the algorithm minimizes q which itself is a function of the coordinates in the embedded space. Optimization is typically performed using gradient descent.

In this exercise, we derive the gradient of the Kullback-Leibler divergence, first with respect to the probability scores qij, and then with respect to the embedding coordinates of which qij is a function.

(a) Show that

∂C =−pij. (1) ∂ qij qij

qij = 􏰃Nk=1 􏰃Nl=1 exp(zkl)

The new variables zij can be interpreted as unnormalized log-probabilities. Show that

∂C =−pij+qij. (2) ∂ zij

(c) Explain which of the two gradients, (1) or (2), is the most appropriate for practical use in a gradient descent algorithm. Motivate your choice, first in terms of the stability or boundedness of the gradient, and second in terms of the ability to maintain a valid probability distribution during training.

(d) The scores zij are now reparameterized as

zij =−∥⃗yi−⃗yj∥2

where the coordinates ⃗yi,⃗yj ∈ Rh of data points in embedded space now appear explicitly. Show using the

</div>
</div>
<div class="layoutArea">
<div class="column">
(b) The probability matrix q is now reparameterized using a ‘softargmax’ function: exp(zij )

</div>
</div>
<div class="layoutArea">
<div class="column">
chain rule for derivatives that

</div>
</div>
<div class="layoutArea">
<div class="column">
∂C 􏰄N

∂⃗y = 4(pij −qij)·(⃗yi −⃗yj).

i j=1

Download the programming files on ISIS and follow the instructions.

</div>
</div>
<div class="layoutArea">
<div class="column">
Exercise 4: Programming (25 P)

</div>
</div>
</div>
<div class="page" title="Page 3">
<div class="section">
<div class="layoutArea">
<div class="column">
Exercise sheet 1 (programming) [SoSe 2021] Machine Learning 2

</div>
</div>
<div class="layoutArea">
<div class="column">
Implementing Locally Linear Embedding (25 P)

In this programming homework we will implement locally linear embedding (LLE) and experiment with it on the swiss roll dataset. In particular, the effects of neighbourhood size and noise on the quality of the embedding will be analyzed.

In [1]:

import numpy as np

import matplotlib

%matplotlib inline

from matplotlib import pyplot as plt

from mpl_toolkits.mplot3d.axes3d import Axes3D import sklearn,sklearn.datasets

Thefollowingcodeplotstheswissrolldataset(acommonlyuseddatasettotestLLE)with N=1000 datapointsandanoiseparameterof 0.25. In [2]:

<pre>X,T = sklearn.datasets.make_swiss_roll(n_samples=1000, noise=0.25)
plt.figure(figsize=(5,5))
ax = plt.gca(projection='3d')
ax.view_init(elev=10., azim=105)
ax.scatter(X[:,0],X[:,1],X[:,2],c=T)
</pre>
plt.show()

Although the dataset is in three dimensions, the points follow a two-dimensional low-dimensional structure. The goal of embedding algorithms is to extract this underlying structure, in this case, unrolling the swiss roll into a two-dimensional Euclidean space.

In the following, we consider a simple implementation of LLE. You are required to complete the code by writing the portion where the optimal reconstruction weights are extracted. (Hint: During computation, you need to solve an equation of the type Cw=1, where 1 is a column vector (1,1,…,1). In case k&gt;d i.e. the size of the neighbourhood is larger than the number of dimensions of the input space, it is necessary to regularize the matrix C. You can do this by adding positive terms on the diagonal. A good starting point is 0.05.)

In [3]:

def LLE(X,k): N = len(X)

<pre>    W = np.zeros([N,N])
</pre>
for i in range(N):

# ————————————– # TODO: Replace by your code

# ————————————– import solution

w,ind = solution.lle(X,i,k)

# ————————————– W[i,ind] = w

<pre>    M = np.identity(N) - W - W.T + np.dot(W.T,W)
    E = np.linalg.svd(M)[0][:,-3:-1]
</pre>
</div>
</div>
<div class="layoutArea">
<div class="column">
return E

</div>
</div>
</div>
</div>
<div class="page" title="Page 4">
<div class="section">
<div class="layoutArea">
<div class="column">
Youcannowtestyourimplementationontheswissrolldatasetandvarythenoiseinthedataandtheparameter k oftheLLEalgorithm.Resultsare shown below:

In [4]:

f = plt.figure(figsize=(12,3))

for t,(k,noise) in enumerate([(2,0.1),(10,0.1),(25,0.1),(10,1)]):

X,T = sklearn.datasets.make_swiss_roll(n_samples=1000, noise=noise) embedding = LLE(X,k=k)

ax = f.add_subplot(1,4,t+1)

ax.set_title(‘k=%d, noise=%.1f’%(k,noise))

<pre>    ax.set_xticks([])
    ax.set_yticks([])
    ax.scatter(embedding[:,0],embedding[:,1],c=T)
</pre>
Itcanbeobservedthattheparameter k mustbecarefullytunedtohavesufficientlymanyneighborsforstabilitybutalsonottoomany.Wecanfurther observe that LLE works well as long as the noise in the data remains low enough.

</div>
</div>
</div>
</div>
<div class="page" title="Page 5"></div>
<div class="page" title="Page 6"></div>
<div class="page" title="Page 7"></div>
<div class="page" title="Page 8"></div>
